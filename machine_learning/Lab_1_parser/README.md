# Лабораторная работа №1. *parser*

**Выбранный сайт**: [EDC.sale](https://edc.sale/ru/ru/real-estate/) (раздел с недвижимостью)  
**dataset**: [dataset.csv](../common_data/dataset1100_Lab_1.csv)  
Всего строк: 1100

Алгоритм обходит указанное количество главных страниц каталога недвижимости (`?page=1..MAX_MAIN_PAGES`), на каждой собирает ссылки на карточки объектов и последовательно обрабатывает.

Некоторые поля парсятся из ссылки и конкретных `div`'ов, часть по поиску с приоритетом: `title` -> `dynprops` -> `description`

При выводе капчи (простая математическая) решает ее, кидает ответ методом из `form` (в моем случае всегда **POST**) и делает опять — исходной страницы с `Referer` = **URL капчи**  
[HTML пример капчи](example_html_pages/captcha_example.html)

---

# Задание

Выберите сайт и напишите программу для краулинга и парсинга его данных. Результатом выполнения программы должен быть датасет с данными с сайта.

**Краулинг** — получение и скачивание списка страниц или элементов, содержащих искомые данные.  
**Парсинг** — извлечение данных из неструктурированного текста. В данном случае из html-кода страниц или элементов страницы.

Добавьте в репозиторий файл с несколькими строками из собранного датасета для примера.

## Датасет должен:
- содержать как минимум 1000 строк
- содержать как минимум 6 переменных из которых:
    - как минимум 2 категориальные
    - как минимум 2 количественные целочисленные
    - как минимум 2 количественные дробные  
      3 количественные + 1 дробная и наоборот - ок
- содержать таргет - признак, который вы будете предсказывать на основании остальных признаков. Таргет может быть:
    - категориальным - для задачи классификации
    - количественным - для задачи регрессии

Таргет должно быть непросто предсказать. Например, предсказать объем коробки по ее длине, ширине, высоте и материалу - легко. Предсказать победу футбольной команды по статистикам прошлых игр и месту проведения матча - сложно.

## Датасет может:
- содержать переменные других типов данных: картинок, аудио, видео, рядов, json’ов
- быть результатом лишь парсинга, если информация собрана с одной страницы и краулить нечего

Пользоваться jupiter notebook-ами в этой лабе необязательно

## Теория
В этой лабе его нет! Вы получите баллы после merge-а вашего Pull Request-а

## Рекомендации
Необязательны, но если сделаете так, вам же будет интереснее, удобнее или полезнее
- размер датасета от 3000 до 1’000’000 строк
- данные других типов хранятся в отдельной папке, в датасете хранятся пути к файлам  
  Например если у каждой записи датасета есть картинка, то хранить в таблице стоит путь к ней. Саму картинку хранить файлом в папке `/pics/`
- количество переменных от 10 до 30
- хранить датасет удобно в формате csv
- выбирайте сайт с умом. На этих данных вы будете обучать алгоритмы, которые реализуете в следующих лабах
- не стесняйтесь задавать вопросы в чат курса
- в некоторых сайтах на карточке объекта написано мало данных. Но много данных на странице, куда ссылается карточка. Скраулите эти страницы, распарсите их. ПолУчите намного больше данных

## Для реализации используйте python библиотеки
- `requests` для краулинга
- `lxml/beautiful soup` для парсинга
- `multiprocessing` для параллелизации скачивания, если скачивание занимает больше 15 минут

Учтите, что у многих сайтов есть rate limit запросов, часто это 2 запроса в секунду. Для ускорения можете использовать [набор прокси](https://stackoverflow.com/questions/36446657/requests-multiple-proxies-python)

## Популярные замечания
- используйте конфиг файл для констант
- используйте класс для выражения сущности и методы класса для извлечения ее параметров. Станет удобнее, красивее и понятнее  
  Например класс `Recipe` и `Recipe.fetch_cook_duration()` для получения длительности приготовления
- используйте `dataclass` или `attrs` для класса сущности  
  Также используйте декоратор `classmethod` например для `Recipe.from_html(string)`
- не коммитить `.idea`
- функции длиннее 20 строк - плохо. Их можно декомпозировать
- датафрейм удобнее создавать с помощью `DataFrame.from_records()`

## FAQ
1. Если нужно проматывать сайт для дозагрузки объектов, то это можно делать программно. Откройте консоль браузера через Inspect code, во вкладке Консоль напишите js код для промотки. Найти такой код в интернете легко

2. Какие ошибки отлавливать? Те, с которыми падает ваше приложение, сильно упарываться не нужно. Ловить базовый `Exception` - плохой подход, стрельнет в ногу быстро

3. Скачал url страниц, а некоторые уже удалили. При парсинге сохраняйте исходный код страницы, а потом отдельно парсите их как файлы. Это позволит разделить скачивание и обработку, ускорит отладку

4. У объекта много картинок/аудио/etc. В датасете указываете адрес папки с картинками
